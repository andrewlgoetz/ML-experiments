{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeb503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_combos():\n",
    "    a =  [\n",
    "    ['FS-I'],\n",
    "    ['FS-II'],\n",
    "    ['FS-III'],\n",
    "    ['FS-IV'],\n",
    "    ['FS-I', 'FS-II'],\n",
    "    ['FS-I', 'FS-III'],\n",
    "    ['FS-I', 'FS-IV'],\n",
    "    ['FS-II', 'FS-III'],\n",
    "    ['FS-II', 'FS-IV'],\n",
    "    ['FS-III', 'FS-IV'],\n",
    "    ['FS-I', 'FS-II', 'FS-III'],\n",
    "    ['FS-I', 'FS-II', 'FS-IV'],\n",
    "    ['FS-I', 'FS-III', 'FS-IV'],\n",
    "    ['FS-II', 'FS-III', 'FS-IV'],\n",
    "    ['FS-I', 'FS-II', 'FS-III', 'FS-IV']\n",
    "    ]\n",
    "    \n",
    "    return a\n",
    "\n",
    "# Hyperparametric tuning\n",
    "# after we find the best feature set from feature_experiment()\n",
    "def tuning(best_fs, data_path='.', period=\"data-2010-15\", k=5, seed=123):\n",
    "   \n",
    "    # Define parameter grid (params to test)\n",
    "    param_grid = {\n",
    "        \"kernel\": [\"rbf\", \"linear\", \"poly\"],\n",
    "        \"gamma\": [\"scale\", 0.1, 0.05, 0.01, 0.005, 0.001],\n",
    "        \"C\": [0.1, 0.5, 1, 5, 10, 30],\n",
    "    }\n",
    "\n",
    "    # Load and preprocess data (only removing NaNs; scaling handled in CV)\n",
    "    ds = my_svm(data_path=data_path, period=period)\n",
    "    X, y = ds.preprocess(best_fs)\n",
    "\n",
    "    # fold data\n",
    "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Including a sclara in pipeline to avoid leakage during tuning\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svc\", SVC())\n",
    "    ])\n",
    "\n",
    "    # SCORER FUNCTION for GridSearch\n",
    "    def tss_score(y_true, y_pred):\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "        tpr = tp / max(tp + fn, 1)\n",
    "        fpr = fp / max(fp + tn, 1)\n",
    "        return tpr - fpr\n",
    "    scorer = make_scorer(tss_score)\n",
    "\n",
    "    # Perform grid search\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid={\n",
    "            \"svc__C\": param_grid[\"C\"],\n",
    "            \"svc__gamma\": param_grid[\"gamma\"],\n",
    "            \"svc__kernel\": param_grid[\"kernel\"],\n",
    "        },\n",
    "        scoring=scorer, cv=folds, n_jobs=None, refit=True\n",
    "    )\n",
    "\n",
    "    print(\"\\nPerforming GridSearchCV hyperparameter tuning...\")\n",
    "    grid.fit(X, y)\n",
    "\n",
    "    print(\"\\nBest Parameters found:\")\n",
    "    print(grid.best_params_)\n",
    "    print(f\"Best CV TSS Score: {grid.best_score_:.4f}\")\n",
    "\n",
    "    # Extract SVC parameters (remove 'svc__' prefix)\n",
    "    best_params = {\n",
    "        \"C\": grid.best_params_[\"svc__C\"],\n",
    "        \"gamma\": grid.best_params_[\"svc__gamma\"],\n",
    "        \"kernel\": grid.best_params_[\"svc__kernel\"]\n",
    "    }\n",
    "\n",
    "    return best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e805cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_svm():\n",
    "    def __init__(self, data_path='.', period=\"data-2010-15\"):\n",
    "        self.data_dir = os.path.join(data_path, period)\n",
    "        self.neg_main  = os.path.join(self.data_dir, \"neg_features_main_timechange.npy\")  # FS-I(0:18), FS-II(18:90)\n",
    "        self.pos_main  = os.path.join(self.data_dir, \"pos_features_main_timechange.npy\")\n",
    "        self.neg_hist  = os.path.join(self.data_dir, \"neg_features_historical.npy\")       # FS-III(0:1)\n",
    "        self.pos_hist  = os.path.join(self.data_dir, \"pos_features_historical.npy\")\n",
    "        self.neg_mm    = os.path.join(self.data_dir, \"neg_features_maxmin.npy\")           # FS-IV(0:18)\n",
    "        self.pos_mm    = os.path.join(self.data_dir, \"pos_features_maxmin.npy\")\n",
    "        self.order_fn  = os.path.join(self.data_dir, \"data_order.npy\")\n",
    "        \n",
    "        # class sizes\n",
    "        self.n_neg = None\n",
    "        self.n_pos = None\n",
    "\n",
    "        self.hyperparams = {\"C\": 1.0, \"kernel\": \"rbf\", \"gamma\": \"scale\"}\n",
    "        pass\n",
    "\n",
    "    def preprocess(self,fs_value):\n",
    "        X_raw = self.feature_creation(fs_value)\n",
    "        \n",
    "        #apply numpy ordering\n",
    "        order =np.load(self.order_fn).astype(int).ravel()\n",
    "        X = X_raw[order, :]\n",
    "\n",
    "        #labels (0 for NEG, 1 for POS)\n",
    "        y_labels = np.concatenate([np.zeros(self.n_neg, dtype=int),\n",
    "                                 np.ones(self.n_pos, dtype=int)])\n",
    "        y = y_labels[order]\n",
    "\n",
    "        # drop rows with invalid values\n",
    "        finite = np.isfinite(X)\n",
    "        row_ok = finite.all(axis=1)\n",
    "        X = X[row_ok, :]\n",
    "        y = y[row_ok]\n",
    "\n",
    "        # Normalization handled in cross_validation\n",
    "        # Compute μ/σ on the training set once, store them, and reuse those for val/test.\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def feature_creation(self, fs_value):\n",
    "         \n",
    "        fs_value = [s.upper() for s in fs_value]\n",
    "        \n",
    "        neg_main = np.load(self.neg_main)\n",
    "        pos_main = np.load(self.pos_main)\n",
    "        neg_hist = np.load(self.neg_hist)\n",
    "        pos_hist = np.load(self.pos_hist)\n",
    "        neg_mm   = np.load(self.neg_mm)\n",
    "        pos_mm   = np.load(self.pos_mm)\n",
    "\n",
    "        fs_i_neg  = neg_main[:, 0:18]   if \"FS-I\"  in fs_value else None\n",
    "        fs_i_pos  = pos_main[:, 0:18]   if \"FS-I\"  in fs_value else None\n",
    "        fs_ii_neg = neg_main[:, 18:90]  if \"FS-II\" in fs_value else None\n",
    "        fs_ii_pos = pos_main[:, 18:90]  if \"FS-II\" in fs_value else None\n",
    "\n",
    "        fs_iii_neg = neg_hist[:, 0:1]   if \"FS-III\" in fs_value else None\n",
    "        fs_iii_pos = pos_hist[:, 0:1]   if \"FS-III\" in fs_value else None\n",
    "\n",
    "        fs_iv_neg  = neg_mm[:, 0:18]    if \"FS-IV\" in fs_value else None\n",
    "        fs_iv_pos  = pos_mm[:, 0:18]    if \"FS-IV\" in fs_value else None\n",
    "\n",
    "        neg_parts = []\n",
    "        pos_parts = []\n",
    "\n",
    "        for part in [fs_i_neg, fs_ii_neg, fs_iii_neg, fs_iv_neg]:\n",
    "            if part is not None:\n",
    "                neg_parts.append(part)\n",
    "\n",
    "        for part in [fs_i_pos, fs_ii_pos, fs_iii_pos, fs_iv_pos]:\n",
    "            if part is not None:\n",
    "                pos_parts.append(part)\n",
    "\n",
    "        if len(neg_parts) == 1:\n",
    "            neg_block = neg_parts[0]\n",
    "        else:\n",
    "            neg_block = np.hstack(neg_parts)\n",
    "\n",
    "        if len(pos_parts) == 1:\n",
    "            pos_block = pos_parts[0]\n",
    "        else:\n",
    "            pos_block = np.hstack(pos_parts)\n",
    "\n",
    "        self.n_neg = neg_block.shape[0]\n",
    "        self.n_pos = pos_block.shape[0]\n",
    "\n",
    "        # Stack NEG first, then POS\n",
    "        X_raw = np.vstack([neg_block, pos_block])\n",
    "        return X_raw    \n",
    "\n",
    "    def cross_validation(self,fs_value):\n",
    "         \n",
    "        seed = 123\n",
    "        k=5 #  k = 5 not 10\n",
    "        \n",
    "        # get the ordered & processed data:\n",
    "        X, Y = self.preprocess(fs_value)\n",
    "\n",
    "        # instatiate the folds, using stratfied K Fold for class balance\n",
    "        fold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        tss_list = []\n",
    "        cm = np.array([[0, 0],[0, 0]], dtype=int)  # [[TN,FP],[FN,TP]]\n",
    "\n",
    "        #fold.split returns the 5 splits of train,test indice tuples\n",
    "        for train, test in fold.split(X,Y):\n",
    "\n",
    "            # seperate data into folds \n",
    "            X_train, X_test = X[train], X[test]\n",
    "            y_train, y_test = Y[train], Y[test]\n",
    "\n",
    "            # normalization (fit on train, apply to both train + test to avoid data leakage)\n",
    "            mu = X_train.mean(axis=0, keepdims=True)\n",
    "            sd = X_train.std(axis=0, ddof=0, keepdims=True); sd[sd == 0] = 1.0\n",
    "            X_train = (X_train - mu) / sd\n",
    "            X_test = (X_test - mu) / sd\n",
    "\n",
    "            # Train & fit\n",
    "            # train on trainign set,\n",
    "            model = self.training(X_train, y_train)\n",
    "            # and test on test set\n",
    "            y_hat = model.predict(X_test)\n",
    "\n",
    "            # update tss, and data for confusion matrix\n",
    "            tss_val = self.tss(y_test, y_hat)\n",
    "            tss_list.append(tss_val)\n",
    "            cm += confusion_matrix(y_test, y_hat, labels=[0,1])\n",
    "\n",
    "        # return the required metrics\n",
    "        self.metrics = {\n",
    "            \"tss\"          : tss_list,\n",
    "            \"mean_tss\"     : float(np.mean(tss_list)),\n",
    "            \"agg_cm\"        : cm\n",
    "        }\n",
    "\n",
    "        return self.metrics\n",
    "\n",
    "    def training(self, X, Y):\n",
    "         \n",
    "        C = self.hyperparams.get(\"C\")\n",
    "        gamma = self.hyperparams.get(\"gamma\")\n",
    "        kernel = self.hyperparams.get(\"kernel\")\n",
    "\n",
    "\n",
    "        svm = SVC(C=C, gamma = gamma, kernel=kernel) # gamma = 'scale'\n",
    "        svm.fit(X, Y)\n",
    "        return svm\n",
    "\n",
    "    def tss(self,y_true, y_pred):\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "        tpr = tp / max(tp + fn, 1)\n",
    "        fpr = fp / max(fp + tn, 1)\n",
    "        return (tpr - fpr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70876b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_experiment(data_path='.', period=\"data-2010-15\"):\n",
    "     \n",
    "    k = 5\n",
    "    combos = get_feature_combos()\n",
    "    confusion_matrices = [] \n",
    "\n",
    "    ## Data for plotting\n",
    "    rows = []               # for DataFrame\n",
    "    tss_allfolds = []       # for the single overlay chart\n",
    "    labels = []   # legend labels\n",
    "       \n",
    "\n",
    "    for fs_list in combos:\n",
    "        # init svm class\n",
    "        ds = my_svm(data_path=data_path, period=period)\n",
    "        metrics = ds.cross_validation(fs_list)  # get the metrics for this feature set\n",
    "\n",
    "        tss_per_fold = np.array(metrics[\"tss\"], dtype=float)\n",
    "        mean_tss = float(np.mean(tss_per_fold))\n",
    "        std_tss  = float(np.std(tss_per_fold, ddof=1)) if tss_per_fold.size > 1 else 0.0\n",
    "\n",
    "        #confusion matrix\n",
    "        cm = metrics[\"agg_cm\"]\n",
    "        tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
    "\n",
    "        # Print metrics for each feature set combo\n",
    "        print(f\"\\nFeature Set: {', '.join(fs_list)}\")\n",
    "        print(f\"Mean TSS: {mean_tss:.4f}, Std Dev: {std_tss:.4f}\")\n",
    "        # Report TP, FP, TN, FN \n",
    "        print(\"Confusion Matrix (aggregated over 5 folds)\")\n",
    "        print(f\"TP={tp}  FP={fp}  TN={tn}  FN={fn}\")\n",
    "\n",
    "        # Store table row\n",
    "        rows.append({\n",
    "            \"feature_set\": \" + \".join(fs_list),\n",
    "            \"mean_tSS\": mean_tss,\n",
    "            \"std_tSS\": std_tss,\n",
    "            \"TP\": tp, \"FP\": fp, \"TN\": tn, \"FN\": fn\n",
    "        })\n",
    "\n",
    "        # Keep for overlay chart\n",
    "        tss_allfolds.append(tss_per_fold) ## append the TSS for this combination\n",
    "        labels.append(\" + \".join(fs_list)) ## add the label\n",
    "        confusion_matrices.append((fs_list, cm)) ## add the CM values\n",
    "\n",
    "        # Plot this combo’s confusion matrix (one for each combination)\n",
    "    \n",
    "        plt.figure(figsize=(3.8, 3.2))\n",
    "        seaborn.heatmap(cm, annot=True, fmt=\"d\", cbar=True, xticklabels=[\"Pred 0\",\"Pred 1\"], yticklabels=[\"True 0\",\"True 1\"])\n",
    "        plt.title(f\"Confusion {', '.join(fs_list)}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # plot fold-wise TSS for all 15 combos\n",
    "    plt.figure(figsize=(7.5, 4.8))\n",
    "    x = np.arange(1, k + 1)\n",
    "    for tss_series, label in zip(tss_allfolds, labels):\n",
    "        # plot a line for each combination\n",
    "        plt.plot(x, tss_series, marker='o', linewidth=1.0, alpha=0.8, label=label)\n",
    "\n",
    "    plt.xlabel(\"Fold\")\n",
    "    plt.ylabel(\"TSS\")\n",
    "    plt.title(\"Fold-wise TSS for All Feature Set Combinations (2010–2015, k=5)\")\n",
    "    # clegend\n",
    "    plt.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), fontsize=8, ncol=1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Pick best by mean TSS\n",
    "    # IDMAX finds the index with the max value\n",
    "    df = pd.DataFrame(rows)\n",
    "    best_row = df.loc[df[\"mean_tSS\"].idxmax()]\n",
    "    best_name = best_row[\"feature_set\"]\n",
    "\n",
    "    # worst\n",
    "    worst_row = df.loc[df[\"mean_tSS\"].idxmin()] # fs-III to get second to worst add a +1 to idxmin()\n",
    "    worst_name = worst_row[\"feature_set\"]   \n",
    "\n",
    "    print(\"\\n************************************\")\n",
    "    print(f\"Best FeatureSet Combination by TSS:\\n {best_name}  had  Mean TSS: {best_row['mean_tSS']:.4f}, Std Dev: {best_row['std_tSS']:.4f}\")\n",
    "    print(\"************************************\")\n",
    "    print(\"\\n************************************\")\n",
    "    print(f\"Worst FeatureSet Combination by TSS:\\n {worst_name}  had  Mean TSS: {worst_row['mean_tSS']:.4f}, Std Dev: {worst_row['std_tSS']:.4f}\")\n",
    "    print(\"************************************\")\n",
    "\n",
    "    ## best feature set used for tuning\n",
    "    return df, best_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27aea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_experiment(best_params, best_fs, data_path='.',):\n",
    "     \n",
    "    print(\"*\"*90)\n",
    "    print(\"*\"*90)\n",
    "    print('\\t DATA EXPERIMENT\\n\\n')\n",
    "\n",
    "    print(f\"\\nUsing best FS from feature_experiment(): {', '.join(best_fs)}\")\n",
    "    print(f\"\\nUsing tuned SVC params: {best_params}\")\n",
    "\n",
    "\n",
    "    out = {}\n",
    "    # 2) Run on both datasets\n",
    "    for period in [\"data-2010-15\", \"data-2020-24\"]:\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(f\"Dataset: {period}\")\n",
    "        print(\"=\"*40)\n",
    "\n",
    "        #build the model with ideal parameters\n",
    "        # use hte data from the given period (2010, 2020)\n",
    "        ds = my_svm(data_path=data_path, period=period)\n",
    "        ds.hyperparams.update(best_params)\n",
    "        # get metrics of CV\n",
    "        metrics = ds.cross_validation(best_fs)\n",
    "\n",
    "        # Mean / std TSS\n",
    "        tss_per_fold = np.array(ds.metrics[\"tss\"], dtype=float)\n",
    "        mean_tss = float(np.mean(tss_per_fold))\n",
    "        std_tss  = float(np.std(tss_per_fold, ddof=1)) if tss_per_fold.size > 1 else 0.0\n",
    "        print(f\"Mean TSS: {mean_tss:.4f}, Std Dev: {std_tss:.4f}\")\n",
    "\n",
    "        # Confusion matrix (aggregated over folds)\n",
    "        cm = ds.metrics[\"agg_cm\"]\n",
    "        tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
    "        print(f\"Confusion Matrix (aggregated over 5 folds):\")\n",
    "        print(f\"TN={tn}  FP={fp}\\nFN={fn}  TP={tp}\")\n",
    "\n",
    "        # Heatmap of confusion matrix\n",
    "        plt.figure(figsize=(3.8, 3.2))\n",
    "        seaborn.heatmap(cm, annot=True, fmt=\"d\", cbar=True,\n",
    "                        xticklabels=[\"Pred 0\",\"Pred 1\"], yticklabels=[\"True 0\",\"True 1\"])\n",
    "        plt.title(f\"Confusion Matrix — {period}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Bar chart of TSS per fold\n",
    "        plt.figure(figsize=(5.2, 3.0))\n",
    "        x = np.arange(1, tss_per_fold.size + 1)\n",
    "        plt.bar(x, tss_per_fold)\n",
    "        plt.xlabel(\"Fold\")\n",
    "        plt.ylabel(\"TSS\")\n",
    "        plt.title(f\"TSS per Fold — {period}\")\n",
    "        plt.xticks(x)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e941de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the feature experiment for all combinations\n",
    "_, feat = feature_experiment()\n",
    "\n",
    "## get the best feature set\n",
    "best_fs = [s.strip() for s in feat.split('+')]\n",
    "\n",
    "## tune hyperparameters\n",
    "params = tuning(best_fs, data_path='.', period=\"data-2010-15\", k=3, seed=123)\n",
    "\n",
    "## Run data experiment with best feature set/hyperparameters on both datasets\n",
    "result = data_experiment(params, best_fs = best_fs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4al3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
